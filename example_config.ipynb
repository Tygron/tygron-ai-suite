{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc4b8dd-8b67-4460-9b77-c2083a6fb2fb",
   "metadata": {},
   "source": [
    "<h1>Example notebook</h1>\n",
    "<p>Welcome to this example notebook for training a model of a Convolutional Neural Network. This trained model can then be exporting it to an ONNX file, which is an exchange file format and stands for \"Open Neural Network Exchange\".</p>\n",
    "<p>The training and export procedures are configured using a Configuration instance and two Dataset instances. Various methods with use these instances to initialize, train and export a Convolutional Neural Network. The classes and methods are defined in the \"inference_training.py\" file.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3a3b6-eae7-40c8-8509-1f1d04fffe57",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>\n",
    "<p>First we will import all functions that we will use in this notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3e71d-3455-405b-974f-f0d980e4b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_training import Configuration, ImageDataset\n",
    "from inference_training import initCudaEnvironment, createTransforms\n",
    "from inference_training import drawImageAndFeatureMasks\n",
    "from inference_training import exportOnnxModel, writeONNXMeta, loadONNX\n",
    "from inference_training import trainModel, saveModel, loadModel\n",
    "from inference_training import createModelInstance, testInference\n",
    "from inference_training import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a92ad3-bbfd-477a-a3aa-3ea959571ecd",
   "metadata": {},
   "source": [
    "<h2>Training environment</h2>\n",
    "<p>With this function we will initialize the cude environment, if present on our workstation.</p><p></p>In case multiple gpu's (cuda devices) are present on your workstations, you can adjust the parameters below to select how many and which ones should be used.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a4c11-5b60-4f45-a504-8be8e706018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initCudaEnvironment(numCudaDevices=1,\n",
    "                    visibleCudaDevices=\"0\",\n",
    "                    clearCudaDeviceCount=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2381707-019b-447e-95cb-68304ff7b9da",
   "metadata": {},
   "source": [
    "<h1>Configuration</h1>\n",
    "<p>Here we initialize a configuration instance, used by the datasets and the creation and training of a model of a Convolution Neural Network.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e5f3c-1705-4443-91d3-83306cb744fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "config = Configuration()\n",
    "print(\"Using Device: \" + str(config.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5c370-a3f2-44ae-ac32-2b8b9db69494",
   "metadata": {},
   "source": [
    "<p>Here we must specify the path to both of your datasets; the test and train set.</p><p>Note that on the Windows operating system, the path is often copied with '\\', which should be converted to '\\\\\\\\' or '/' in python code.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21550852-e9f6-4f2d-98e8-e2d46365b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "trainDirectory = \"/path/to/train/dataset/\"\n",
    "testDirectory = \"/path/to/test/dataset/\"\n",
    "\n",
    "assert os.path.isdir(trainDirectory), f'Train directory does not exists! {trainDirectory}';\n",
    "assert os.path.isdir(testDirectory), f'Test directory does not exists! {testDirectory}';\n",
    "\n",
    "config.setDatasetPaths(trainPath=trainDirectory, testPath=testDirectory)\n",
    "config.setFilePrefix(\"\")\n",
    "config.setModelName(\"example_model\")\n",
    "config.setInputSizes(inputWidth=250, inputHeight=250)\n",
    "config.setInputCellSize(cellSizeM=0.25, minCellSizeM=0.1, maxCellSizeM=0.5)\n",
    "config.setAutoLimitLabel(True)\n",
    "\n",
    "print(\"Model name: \" + str(config.getModelName()))\n",
    "print(\"Version: \" + str(config.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1985fb0-b49d-4ac5-a005-e49abc068578",
   "metadata": {},
   "source": [
    "<p>In this step we will have to define some important parameters; <li><ul>the number of channels of the input images. Normally this is 3, but images can be concatenated when more are exported.</ul><ul>the maximum number of classes that are present in the exported datasets; In case more classes are present in any of the datasets, an error will be thrown when validating the dataset.</ul><ul>Whether features are allowed to overlap or not.</ul<ul>The maximum amount of features per image. By default it is 250, close the maximum byte value. The used model will define the upper limit of this parameter due to limitations of the mask image.</ul><ul>The \"reuseModel\" parameter is used to indicate whether to open a previously trained and stored PyTorch model, if present. Otherwise a new model with default weights will be initialized.</ul></li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30521fd0-34c8-4a19-9315-721de4f4048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfClasses = 1\n",
    "\n",
    "config.setModelInfo(channels=3, #\n",
    "                    numClasses=numberOfClasses+1,  # ( +1 background)\n",
    "                    bboxOverlap=True, #\n",
    "                    bboxPerImage=250, #\n",
    "                    reuseModel=False) #\n",
    "config.setEpochs(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e349c-70dd-45dc-9ccb-5071bf857939",
   "metadata": {},
   "source": [
    "<p>In this configuration step, we define parameters that will be stored in the exported ONNX file. The ONNX file is an exchange file format that can be imported into the Tygron Platform. In later steps of this notebook, we will call methods that will export the trained PyTorch model to an ONNX model using the ONNX libraries.</p>\n",
    "    \n",
    "<p>In this step we configure certain meta data properties, such as legend entries, thresholds and tensornames, in such a s way that the Tygron Platform can identify and configure Tygron data objects directly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f1e58-12d8-4912-b269-49849ad1836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Model description\"\n",
    "config.setOnnxInfo(producer=\"Tygron\", description=description)\n",
    "\n",
    "#Legend entries used by the label result type of an Inference Overlay in the Tygron Platform\n",
    "config.clearLegendEntries()\n",
    "config.addLegendEntry(\"Background\", 0, \"#00000000\")\n",
    "config.addLegendEntry(\"Foliage\", 1, \"#00ffbf\")\n",
    "\n",
    "missingLegendEntries = numberOfClasses + 1 - len(config.getLegendEntries())\n",
    "if missingLegendEntries > 0:\n",
    "    logger.warning(str(missingLegendEntries) + \" Legend Entries are not yet defined! Add more legend entries using \\\"config.addLegendEntry(name, classIndex, webcolor)\\\"\")\n",
    "\n",
    "# Inference Overlay model parameters used when applying this ONNX model.\n",
    "config.setOnnxMetaData(scoreThreshold=0.2,\n",
    "                       maskThreshold=0.3,\n",
    "                       strideFraction=0.5)\n",
    "\n",
    "#Name of the input tensor and the batch amount set when exporting this model to an ONNX file.\n",
    "config.setTensorInfo(tensorName='input_A:RGB_normalized', batchAmount=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abcd88-dede-4268-8ac8-bf298fa58083",
   "metadata": {},
   "source": [
    "<h1>Datasets</h1>\n",
    "<p>In this step we initialize the image datasets. These take the configuration as input, as well as a boolean indicating it is a train set or test set.</p>\n",
    "<p>The final argument are transforms, which are applied to the input images. This results in a larger dataset and a model that can cope better with minor changes in images. You can also define your own list of transformations, just be careful with using random transformations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c49d78-f1a9-4add-b37d-9271325cfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset = ImageDataset(config, True, createTransforms(True))\n",
    "testDataset = ImageDataset(config, False, createTransforms(False))\n",
    "\n",
    "logger.info(\"Train Image count: \"+str(trainingDataset.__len__()))\n",
    "logger.info(\"Test Image count: \"+str(testDataset.__len__()))\n",
    "\n",
    "trainingDataset.validate()\n",
    "testDataset.validate()\n",
    "\n",
    "logger.info(\"Pytorch model name \" + config.getPytorchModelFileName())\n",
    "logger.info(\"Onnx file name \" + config.getOnnxFileName())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfe2a7-b923-4b49-804e-7d09b9181d80",
   "metadata": {},
   "source": [
    "<p>Here we simply test an image of our training dataset to verify that we are using the correct input image. We also highlight each inidividal feature provided by the mask image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09aea7a-9a3a-4ec4-a161-55173a86cdfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imageNumber = 5\n",
    "print(trainingDataset.getLabels(imageNumber))\n",
    "drawImageAndFeatureMasks(config, trainingDataset, imageNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34603d49-7b4f-43fc-a0e6-3efd8f0113ae",
   "metadata": {},
   "source": [
    "<h2>Training and saving the model</h2>\n",
    "<p>In this step we will train and save the model. In the code below, the model and training parameters are automatically created using the configuration file. If you want to specify your own model and training procedure, please check the train model function in the 'inference_training.py' file.</p>\n",
    "<p>Once trained, the model is saved using the configured pytorch model file name.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f639e7-aa3c-4001-82b5-fec4fe86f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainModel(config, trainingDataset, testDataset)\n",
    "saveModel(config, model, path=config.getPytorchModelFileName())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ca06c-9e2a-4497-8666-0614a63084ad",
   "metadata": {},
   "source": [
    "<h2>Validating the trained model</h2>\n",
    "<p>In this step we can validate our trained network by testing it on an image of our test dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042aede8-7ac6-45be-a82d-31c1852f1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "testPrediction = testInference(config, model=model,\n",
    "                               dataset=testDataset, imageNumber=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c12bd-b930-4ee5-92c3-9fdfce6c56ed",
   "metadata": {},
   "source": [
    "<h2>Export to an ONNX file</h2>\n",
    "<p>In this part we will export the onnx model, set the metadata (for use in the Tygron Platform)</p><p>Using the configuration instance, we can simply export the trained model to an ONNX file using this function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ca3f6-abbf-408b-afb3-91084272533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportOnnxModel(config, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04806b2-771c-4432-856a-f2019fe7930e",
   "metadata": {},
   "source": [
    "<p>Here we write the meta data using the configuration as input above. This meta data will be used by the Tygron Platform to automatically fill in legend entries, model parameters and input parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701edd17-f779-4c1a-9c9e-4f50dbbbabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeONNXMeta(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182736d-8f3b-4461-b098-a1a8c463f503",
   "metadata": {},
   "source": [
    "<h2>Validating the ONNX file</h2>\n",
    "<p>In this last step, we will try to load the exported ONNX file and see if the meta data is present.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe11d4-d9b7-4ed1-9473-1379cb506e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = loadONNX(config)\n",
    "print(f\"metadata_props={onnx_model.metadata_props}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
